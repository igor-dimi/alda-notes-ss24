[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Algorithms & Data Structures Notes - SoSe 24",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "02-intro/02-analysis.html",
    "href": "02-intro/02-analysis.html",
    "title": "1  Program Run-time Analysis",
    "section": "",
    "text": "1.1 Reccurence Relations\nConsider a very simple reccurence relation:\n\\[\n    T(n) :=\n    \\begin{cases}\n        1               & n = 1 \\\\\n        n + T(n - 1),   & n &gt; 1\n    \\end{cases}\n\\]\nWith mathematical induction we can formally show that \\(T(n)\\) is quadratic. But there is a simpler & more intuitive way:\n\\[\\begin{align*}\n    T(n) &= n + T(n - 1)            \\tag{Def $T(\\cdot)$} \\\\\n         &= n + n - 1 + T(n - 2)  \\\\\n         &= \\dots \\tag{Repeat $n - 2$ times}\\\\\n         &= n + n - 1 + \\dots + T(1) \\\\\n         &= n + n - 1 + \\dots + 1 \\tag{Def $T(1)$} \\\\\n         &= \\frac{n(n + 1)}{2} \\tag{Gauss}\\\\\n         &\\in \\mathcal{O}(n^2)\n\\end{align*}\\]\nThis method can be applied to the more complex divide-and-conquer reccurence relation from the lecture:\n\\[\n    R(n) :=\n    \\begin{cases}\n        a, &n = 1 \\\\\n        c\\dot n + d\\cdot R(\\frac{n}{b}), &n &gt; 1\n    \\end{cases}\n\\]\nApplying the above method we expand \\(R(\\cdot)\\) repetitively according to its definition until we reach the base case, rearranging terms when necessary:\n\\[\\begin{align*}\n    R(n) &= c\\cdot n + d\\cdot R(\\frac{n}{b}) \\tag{Def $R(\\cdot)$} \\\\\n         &= c\\cdot n + d\\bigl(c\\frac{n}{b} + d\\cdot R(\\frac{n}{b^2})\\bigr)  \\\\\n         &= c\\cdot n + d\\Bigl(c\\frac{n}{b} + d\\cdot \\bigl(c\\cdot \\frac{n}{b^2} + d\\cdot R(\\frac{n}{b^2})\\bigr)\\Bigr)  \\\\\n         &= c\\cdot n + d\\cdot c\\frac{n}{b} + d^2c\\frac{n}{b^2} + d^3\\cdot R(\\frac{n}{b^3}) \\tag{Rearrange} \\\\\n         &= c\\cdot n \\left(1 + \\frac{d}{b} + \\frac{d^2}{b^2}\\right) + d^3\\cdot R(\\frac{n}{b^3}) \\\\\n         &= \\dots \\tag{Repeat $k$-times} \\\\\n         &= c\\cdot n\\left(1 + \\frac{d}{b} + \\dots + \\frac{d^{k - 1}}{b^{k - 1}}\\right) + d^k \\cdot R(\\frac{n}{b^k}) \\\\\n         &= c\\cdot n \\sum_{i = 0}^{k - 1}\\left(\\frac{d}{b}\\right)^i + d^k \\cdot R(\\frac{n}{b^k}) \\\\\n         &= c\\cdot n \\sum_{i = 0}^{k - 1}\\left(\\frac{d}{b}\\right)^i + d^k \\cdot R(1) \\tag{Ass $\\frac{n}{b^k} = 1$} \\\\\n         &= c\\cdot n \\sum_{i = 0}^{k - 1}\\left(\\frac{d}{b}\\right)^i + a\\cdot d^k \\tag{Def $R(1)$}\n\\end{align*}\\]\nSee lecture slides for the complexity analysis of final expression.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Program Run-time Analysis</span>"
    ]
  },
  {
    "objectID": "02-intro/02-analysis.html#master-theorem",
    "href": "02-intro/02-analysis.html#master-theorem",
    "title": "1  Program Run-time Analysis",
    "section": "1.2 Master Theorem",
    "text": "1.2 Master Theorem\nFor reccurence relations of the form:\n\\[\n    T(n) :=\n    \\begin{cases}\n        a, &n = 1 \\\\\n        b\\cdot n + c\\cdot T(\\frac{n}{d}), &n &gt; 1\n    \\end{cases}\n\\]\nMaster theorem gives the solutions:\n\\[\n    T(n) =\n    \\begin{cases}\n        \\Theta(n), &c &lt; d \\\\\n        \\Theta(n\\log(n)), &c = d \\\\\n        \\Theta(n^{\\log_b(d)}), &c &gt; d \\\\\n    \\end{cases}\n\\]\nExample: Merge Sort.\nComplexity of merge sort satisfies the reccurence relation:\n\\[\\begin{align*}\n&T(1) = 1 \\\\\n&T(n) = \\mathcal{O}(n) + 2\\cdot T(\\frac{n}{2})\n\\end{align*}\\]\nThus with \\(c = 2 = d\\) the second case of MT applies: \\(T(n) = \\Theta(n\\log n)\\)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Program Run-time Analysis</span>"
    ]
  },
  {
    "objectID": "02-intro/02-analysis.html#amortized-analysis",
    "href": "02-intro/02-analysis.html#amortized-analysis",
    "title": "1  Program Run-time Analysis",
    "section": "1.3 Amortized Analysis",
    "text": "1.3 Amortized Analysis",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Program Run-time Analysis</span>"
    ]
  },
  {
    "objectID": "03/01-lists.html",
    "href": "03/01-lists.html",
    "title": "2  Lists",
    "section": "",
    "text": "2.1 Sequences as Arrays and Lists\nMany terms for same thing: sequence, field, list, stack, string, file… Yes, files are simply sequences of bytes!\nthree views on lists:",
    "crumbs": [
      "Data Structures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lists</span>"
    ]
  },
  {
    "objectID": "03/01-lists.html#sequences-as-arrays-and-lists",
    "href": "03/01-lists.html#sequences-as-arrays-and-lists",
    "title": "2  Lists",
    "section": "",
    "text": "abstract: \\((2, 3, 5, 7)\\)\nfunctionality: stack, queue, etc… What operations does it support?\nrepresentation: How is the list represented in a given programming model/language/paradigm?",
    "crumbs": [
      "Data Structures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lists</span>"
    ]
  },
  {
    "objectID": "03/01-lists.html#applications-of-lists",
    "href": "03/01-lists.html#applications-of-lists",
    "title": "2  Lists",
    "section": "2.2 Applications of Lists",
    "text": "2.2 Applications of Lists\n\nStoring and processing any kinds of data\nConcrete representation of abstract data types such as: set, graph, etc…",
    "crumbs": [
      "Data Structures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lists</span>"
    ]
  },
  {
    "objectID": "03/01-lists.html#linked-and-doubly-linked-lists",
    "href": "03/01-lists.html#linked-and-doubly-linked-lists",
    "title": "2  Lists",
    "section": "2.3 Linked and Doubly Linked Lists",
    "text": "2.3 Linked and Doubly Linked Lists\n\n\n\n\nsimply linked\ndoubly linked\n\n\n\n\nlecture\nSList\nList\n\n\nc++\nstd::forward_list\nstd::list\n\n\n\nDoubly linked lists are usually simpler and require “only” double the space at most. Therefore their use is more widespread.\n\nList Items\nClass Item of T :=\n    e: T //Data item of type T\n    next: *Item //Pointer to Item\n    prev: *Item //Pointer to Item\n    invariant next-&gt;prev = this = prev-&gt;next\n\nProblem: * predeccessor of first list element? * successor of last list element?\nSolution: Dummy Item with an empty data field as follows:\n\nAdvatanges of this solution:\n\nInvariant is always satisfied\nExceptions are avoided, thus making the coding more:\n\nsimple\nreadable\nfaster\nelegant\n\n\nDisadvantages: a little more storage space.\n\n\nThe List Class\nClass List of T :=\n    dummy := (\n        Null : T\n        &dummy : *T // initially list is empty, therefore next points to the dummy itself\n        &dummy : *T // initially list is empty, therefore prev points to the dummy itself\n    ) : Item\n\n    // returns the address of the dummy, which represents the head of the list\n    Function head() : *Item :=\n        return address of dummy\n    \n    // simple access functions\n    // returns true iff list empty\n    Function is_empty() : Bool := \n        return dummy.next == dummy \n\n    // returns pointer to first Item of the list, given list is not empty\n    Function first() : *Item :=\n        assert (not is_empty())\n        return dummy.next\n    \n    // returns pointer to last Item of the list, given list is not empty\n    Function last() : *Item :=\n        assert (not is_empty())\n        return dummy.prev\n\n    /* Splice is an all-purpose tool to cut out parts from a list\n       Cut out (a, ... b) form this list and insert after t */\n    Procedure splice(a, b, t : *Item) := \n        assert (\n            b is not before a \n            and \n            t not between a and b\n        )\n        // Cut out (a, ... , b)\n        a-&gt;prev-&gt;next := b-&gt;next\n        b-&gt;next-&gt;prev := a-&gt;prev\n\n        // insert (a, ... b) after t\n        t-&gt;next-&gt;prev := b\n        b-&gt;next := t-&gt;next\n        t-&gt;next := a\n        a-&gt;prev := t\n\n    \n    // Moving items by utilising splice\n    //Move item a after item b\n    Procedure move_after(a, b: *Item) := \n        splice(a, a, b)\n    \n    // Move item a to the front of the list\n    Procedure move_to_front(a: *Item) := \n        move_after(a, dummy)\n\n    Procedure move_to_back(a: *Item) :=\n        move_after(b, last())\n\n    // Deleting items by moving them to a global freeList\n    // remove item a\n    Procedure remove(a: *Item) :=\n        move_after(b, freeList.dummy)\n    \n    // remove first item\n    Procedure pop_front() :=\n        remove(first())\n\n    //remove last item\n    Procedure pop_back() :=\n        remove(last())\n\n    // Inserting Elements\n    // Insert an item with value x after item a\n    Function insert_after(x : T, a : *Item) : *Item := \n        checkFreeList() //make sure freeList is non empty\n        b := freeList.first() // obtain an item b to hold x\n        move_after(b, a) // insert b after a\n        b-&gt;e := x // set the data item value of b to x\n        return b\n\n    // Manipulating whole lists\n    Procedure concat(L : List) :=\n        splice(L.first(), L.last(), last()) //move whole of L after last element of this list\n\n    Procedure clear()\n        freeList.concat(this) //after this operation from from first to last element of this\n                              // list are concatenated to the freeList, leaving only the \n                              // dummy element in this list.\n\n    Fuction get(i )\n\n    \n\n    \n\nSplicing\nThe code for splicing of the List class:\n/* Splice is an all-purpose tool to cut out parts from a list\n    Cut out (a, ... b) form this list and insert after t */\nProcedure splice(a, b, t : *Item) := \n    assert (\n        b is not before a \n        and \n        t not between a and b\n    )\n    // Cut out (a, ... , b)\n    a-&gt;prev-&gt;next := b-&gt;next\n    b-&gt;next-&gt;prev := a-&gt;prev\n\n    // insert (a, ... b) after t\n    t-&gt;next-&gt;prev := b\n    b-&gt;next := t-&gt;next\n    t-&gt;next := a\n    a-&gt;prev := t\n\nDlist cut-out \\((a,...,b)\\) (see Figure 2.1):\n\n\n\n\n\n\n\nFigure 2.1: cutout\n\n\n\n\nDlist insert \\((a,...,b)\\) after \\(t\\) (see Figure 2.2):\n\n\n\n\n\n\n\nFigure 2.2: insert\n\n\n\n\n\nSpeicherverwaltung ./.FreeList\nMethods (?):\n\nNiavely: allocate memory for each new element, deallocate memory after deleting each element:\n\nadvantage: simplicity\ndisadvantage: requires a good implementation of memory management: potentially very slow\n\n“global” freeList (e.g. static member in C++)\n\ndoubly linked list of all not used elements\ntransfer ‘deleted’ elements in freeList.\ncheckFreeList allocates, in case the list is empty\n\n\nReal implementations: * naiv but with well implemented, efficient memory management * refined Free List Approach (class-agnostic, release) * implementation-specific.\n\n\nDeleting Elements\nDeleting elements realised by moving them to the global freeList:\nProcedure remove(a: *Item) :=\n    move_after(a, freeList.dummy) // item a is now a 'free' item. \n\nProcedure pop_front() :=\n    remove(first())\n\nProcedure pop_back() :=\n    remove(last())\n\n\nInserting Elements\nInserting elements into a list \\(l\\) also utilizes freeList, by fetching its first element an moving it into \\(l\\).\nFunction insert_after(x : T, a : *Item) : *Item := \n    checkFreeList() //make sure freeList is non empty\n    b := freeList.first() // obtain an item b to hold x\n    move_after(b, a) // insert b after a\n    b-&gt;e := x // set the data item value of b to x\n    return b\n\nFunction insert_before(x : T, b : *Item) : *Item :=\n    return insert_after(x, b-&gt;prev)\n\nProcedure push_front(x : T) :=\n    insert_after(x, dummy)\n\nProcedure push_back(x : T) :=\n    insert_after(x, last())\n\n\nManipulating whole Lists\n// Manipulating whole lists\nProcedure concat(L : List) :=\n    splice(L.first(), L.last(), last()) //move whole of L after last element of this list\n\nProcedure clear()\n    freeList.concat(this) //after this operation from from first to last element of this\n                            // list are concatenated to the freeList, leaving only the \n                            // dummy element in this list.\nThis operations require constant time - indeendent of the list size!",
    "crumbs": [
      "Data Structures",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lists</span>"
    ]
  },
  {
    "objectID": "03/02-arrays.html",
    "href": "03/02-arrays.html",
    "title": "3  Arrays",
    "section": "",
    "text": "3.1 Bounded Arrays\nBounded arrays have fixed size and are an efficient data structure.",
    "crumbs": [
      "Data Structures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Arrays</span>"
    ]
  },
  {
    "objectID": "03/02-arrays.html#bounded-arrays",
    "href": "03/02-arrays.html#bounded-arrays",
    "title": "3  Arrays",
    "section": "",
    "text": "Size must be known during compile time and is fixed.\nIts memory location in the stack allows many compiler optimizations.",
    "crumbs": [
      "Data Structures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Arrays</span>"
    ]
  },
  {
    "objectID": "03/02-arrays.html#unbounded-arrays",
    "href": "03/02-arrays.html#unbounded-arrays",
    "title": "3  Arrays",
    "section": "3.2 Unbounded Arrays",
    "text": "3.2 Unbounded Arrays\nThe size of an unbounded array can dynamically change during run-time. From the user POV it provides the same behaviour as a linked list.\nIt allows the operations:\n\npushBack(e: T): insert an element at the end of the array\npopBack(e: T): remove an element at the end of the array\n\n\nMemory Management\n\nallocate(n): request a \\(n\\) contigious blocks of memory words and returns the address value of the first block. This we have the memory blocks:\n\n\n\narray memory allocation\n\n\nwhere ptr + i addresses are determined by pointer arithmetic.\ndispose(ptr) marks the memory address value held in ptr as free, effectively deleting the object held there.\n\nIn general, the allocated memory can’t grow dynamically during life time, since the immediate memory block after the last one might get unpredictably occupied \\(\\Rightarrow\\) If we need a new memory block of size \\(n' &gt; n\\), we must allocate a new block, copy the old block contents, and finally free it.\n\n\nImplementation\nFirst we consider a slow variant:\nClass UArraySlow&lt;T&gt;:=\n  c := 0 : Nat // capacity\n  b : Array[0..c-1]&lt;T&gt; // the array itself\n\n  pushBack(el : T) : void :=\n    // c++\n    // allocate new array on heap with new capacity\n    // copy elements over from the old array\n    // insert el at the last location\n  \n  popBack() : void := \n    // analagous\nProblem: \\(n\\) pushBack operations require \\(1 + \\dots + n \\in \\mathcal{O}(n^2)\\) time \\(\\Rightarrow\\) slow.\nSolution:\n\nUnbounded Arrays with Extra Memory\nIdea: Request more memory than initial capacity. Reallocate memory only when array gets full or too empty:\nAlgorithm design principle: make common case fast.\nClass UArray&lt;T&gt; := \n  c := 1 : Nat // capacity\n  n := 0 : Nat // number of elements in the array\n\n  //invariant n &lt;= c &lt; k*n || (n == 0 && c &lt; 2)\n  b : Array[0..c-1]&lt;T&gt;\n\n  // Array access\n  Operator [i : Nat] : T := \n    assert(0 &lt;= i &lt; n)\n    return b[i]\n\n  // accessor method for n\n  Function size() : Nat := return n\n\n  Procedure pushBack(e : T) := \n    if n == c :\n      reallocate(2*n) // see definition below\n    b[n] := e\n    n++\n\n  // reallocates a new memory with a given capacity c_new\n  Procedure reallocate(c_new : Nat) := \n    c := c_new \n    b_new := new Array[0..c_new - 1]&lt;T&gt;\n    //copy elements over to new array\n    for (i = 1 to n - 1) :\n      b_new[i] := b[i]\n    dispose(b)\n    b := b_new \n\n  Procedure popBack() :=\n    // don't do anything for empty arrays\n    assert n &gt; 0\n    n--\n    if 4*n &lt;= c && n &gt; 0 :\n      reallocate(2*n)",
    "crumbs": [
      "Data Structures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Arrays</span>"
    ]
  },
  {
    "objectID": "05/01-sorting.html",
    "href": "05/01-sorting.html",
    "title": "4  Sorting and Priority Queues",
    "section": "",
    "text": "4.1 Sorting Algorithms",
    "crumbs": [
      "Data Structures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sorting and Priority Queues</span>"
    ]
  },
  {
    "objectID": "05/01-sorting.html#sorting-algorithms",
    "href": "05/01-sorting.html#sorting-algorithms",
    "title": "4  Sorting and Priority Queues",
    "section": "",
    "text": "Insertion Sort\n\ndef insertion_sort(a) :\n    n = len(a)\n    # i = 1\n    # sorted a[0..i-1]\n    for i in range(1, n) :\n        # insert i in the right position\n        j = i - 1\n        el = a[i]\n        while el &lt; a[j] and j &gt; 0 :\n            a[j + 1] = a[j]\n            j = j - 1\n        # el &gt;= a[j] or j == 0\n        if el &lt; a[j] : # j == 0\n            a[1] = a[0]\n            a[0] = el\n        else : # el &gt;= a[j] \n            a[j + 1] = el\n    return a\n\ntesting insertion sort for some inputs:\n\nimport numpy as np\nfor i in range (2, 8) :\n    randarr = np.random.randint(1, 20, i)\n    print(\"in:  \", randarr)\n    print(\"out: \", insertion_sort(randarr))\n\nin:   [17 10]\nout:  [10 17]\nin:   [15 12  6]\nout:  [ 6 12 15]\nin:   [ 2 10 18 17]\nout:  [ 2 10 17 18]\nin:   [15 18  9  2 17]\nout:  [ 2  9 15 17 18]\nin:   [3 9 8 3 9 4]\nout:  [3 3 4 8 9 9]\nin:   [ 4  6  1 14 12  8  1]\nout:  [ 1  1  4  6  8 12 14]\n\n\nFollowing illustrates the state after each insertion (ith iteration):\n\ndef insertion_sort_print(a) :\n    n = len(a)\n    # i = 1\n    # sorted a[0..i-1]\n    for i in range(1, n) :\n        # insert i in the right position\n        j = i - 1\n        el = a[i]\n        while el &lt; a[j] and j &gt; 0 :\n            a[j + 1] = a[j]\n            j = j - 1\n        # el &gt;= a[j] or j == 0\n        if el &lt; a[j] : # j == 0\n            a[1] = a[0]\n            a[0] = el\n        else : # el &gt;= a[j] \n            a[j + 1] = el\n        print(\"after insertion \", i, \": \", a)\n    # return a\n\n\na = np.random.randint(-20, 20, 8)\nprint(\"input:               \", a)\ninsertion_sort_print(a)\n\ninput:                [-16 -19  18  17  18 -17 -17 -18]\nafter insertion  1 :  [-19 -16  18  17  18 -17 -17 -18]\nafter insertion  2 :  [-19 -16  18  17  18 -17 -17 -18]\nafter insertion  3 :  [-19 -16  17  18  18 -17 -17 -18]\nafter insertion  4 :  [-19 -16  17  18  18 -17 -17 -18]\nafter insertion  5 :  [-19 -17 -16  17  18  18 -17 -18]\nafter insertion  6 :  [-19 -17 -17 -16  17  18  18 -18]\nafter insertion  7 :  [-19 -18 -17 -17 -16  17  18  18]\n\n\n\n\nSelection Sort\nbasic idea: repeatedly find the smallest element in the unsorted tail region and move it to the front (via swapping).\nexplanation:\n\n\n\nselection sort\n\n\npython implementation:\n\ndef selection_sort(a) :\n    N = len(a)\n    j = 0\n    # sorted a[0..j-1] && a[0..j-1] &lt;= a[j..N-1]\n    while (j &lt; N) :\n        # find min a[j..N-1]\n        k = j\n        min = a[k]\n        i = j + 1\n        #a[k] == min == min(a[j .. i - 1])\n        while (i &lt; N) :\n            if a[i] &lt; min :\n                min = a[i]\n                k = i\n            i = i + 1\n        # k = j + find_min(a[j :])\n        a[j], a[k] = a[k], a[j]\n        j = j + 1\n    return a\n\nwe test this on some random arrays:\n\nfor i in range (2, 8) :\n    randarr = np.random.randint(-50, 50, i)\n    print(\"in:  \", randarr)\n    print(\"out: \", selection_sort(randarr))\n\nin:   [22 23]\nout:  [22 23]\nin:   [  0 -46  47]\nout:  [-46   0  47]\nin:   [-47  45 -39  17]\nout:  [-47 -39  17  45]\nin:   [  4 -47 -37 -16  39]\nout:  [-47 -37 -16   4  39]\nin:   [ 19  35 -11   4  49 -35]\nout:  [-35 -11   4  19  35  49]\nin:   [ -1  33 -28 -45 -30 -48  19]\nout:  [-48 -45 -30 -28  -1  19  33]\n\n\n\n\nBubble Sort\nLet a : Array[0..N-1]&lt;Nat&gt;. The bubble operation pushes the largest element to the end of the array:\n\ndef bubble(a) :\n    N = len(a)\n    i = 0\n    # a[i] == max(a[0..i])\n    while i &lt; N - 1:\n        if a[i] &gt; a[i + 1] :\n            a[i], a[i+1] = a[i+1], a[i]\n        i = i + 1\n    # post-loop: i == N - 1\n    return a \n\nbubble([-5, 10, 1, 3, 7, -2])\n\n[-5, 1, 3, 7, -2, 10]\n\n\nThe code of this function is used inside bubble_sort():\n\ndef bubble_sort(a) :\n    N = len(a)\n    j = N\n    swapped = False\n    while True : # emulate do while loop\n        # sorted a[j .. N - 1] and a[0..j-1] &lt;= a[j .. N - 1] \n        while j &gt; 0 :\n            i = 0\n            while i &lt; j - 1 :\n                if a[i] &gt; a[i + 1] :\n                    a[i], a[i + 1] = a[i + 1], a[i]\n                i = i + 1\n            j = j - 1\n        if not swapped : break # if no swaps performed at all, array already\n                               # sorted \n    return a\n\nWe test on some arrays:\n\nfor i in range (2, 8) :\n    randarr = np.random.randint(-50, 50, i)\n    print(\"in:  \", randarr)\n    print(\"out: \", bubble_sort(randarr))\n\nin:   [-26  -6]\nout:  [-26  -6]\nin:   [-46  24 -17]\nout:  [-46 -17  24]\nin:   [ -9 -37   8 -20]\nout:  [-37 -20  -9   8]\nin:   [24  3 38 41 -1]\nout:  [-1  3 24 38 41]\nin:   [ 11  23  41 -13  39   1]\nout:  [-13   1  11  23  39  41]\nin:   [-20  13 -12  -5 -22 -33 -26]\nout:  [-33 -26 -22 -20 -12  -5  13]\n\n\nVisual expalantion:\n\n\n\nbubble sort\n\n\n\n\nMerge Sort\nThe input sequence is recursively divided into two sequnces of equal size. It is a straightforward application of the divide-and-conquer principle (recursion).\nA helper function for merging two sorted sequences into one single sorted sequence is necessary.\nPseudocode:\nFunction merge&lt;T&gt;(a, b : Sequence of T) := (\n    c := &lt;&gt;\n    // invariant: sorted(a) && \n    //            sorted(b) && \n    //            sorted(c) && \n    //            c &lt;= a && c &lt;= b\n    while true :\n        if empty(a) : concat(c, b); return c;\n        if empty(b) : concat(c, a); return c;\n        if a.first() &lt;= b.first() : c.moveBack(a.first());\n        else c.moveBack(b.first())\n)\nGiven by the following python implementation:\n\ndef merge(a, b) :\n    # assert: a and b are sorted\n    c = []\n    n1 = len(a)\n    n2 = len(b)\n    k1 = 0\n    k2 = 0\n    i = 0\n    # invariant: merged a[0..k1 - 1] with b[0..k2 - 2]\n    while k1 &lt; n1 and k2 &lt; n2 :\n        if a[k1] &lt;= b[k2] :\n            c.append(a[k1])\n            k1 = k1 + 1\n        else :\n            c.append(b[k2])\n            k2 = k2 + 1\n    # k1 &gt;= n1 or k2 &gt;= n2\n    if k1 == n1 :\n        while k2 &lt; n2 :\n            c.append(b[k2])\n            k2 = k2 + 1\n    if k2 == n2 :\n        while k1 &lt; n1 :\n            c.append(a[k1])\n            k1 = k1 + 1\n    return c\n        \ndef merge_sort(a) :\n    if len(a) == 1 : return a[0:1]\n    n = len(a)\n    a1 = a[0 : n // 2]\n    a2 = a[n // 2 : ]\n    return  merge(merge_sort(a1), merge_sort(a2))\n\nWe test on some arrays:\n\nfor i in range (2, 8) :\n    randarr = np.random.randint(-20, 20, i)\n    print(\"in:  \", randarr)\n    print(\"out: \", merge_sort(randarr))\n\nin:   [13  0]\nout:  [0, 13]\nin:   [ 11  -9 -16]\nout:  [-16, -9, 11]\nin:   [  2 -17   4 -16]\nout:  [-17, -16, 2, 4]\nin:   [ 4 18  5 -4  7]\nout:  [-4, 4, 5, 7, 18]\nin:   [-13  -3  16  17 -15  12]\nout:  [-15, -13, -3, 12, 16, 17]\nin:   [ 18 -19   1 -16 -19  19   1]\nout:  [-19, -19, -16, 1, 1, 18, 19]\n\n\n\n\nQuick Sort\n\nsimilar to merge sort in that it is also a recursive divide and conquer algorithm\nadvantage over mergesort: no temporary arrays to hold partial results.\na pivot element is selected and the array is repetitively partitioned into regions so that all elements in the left region is are no larger than the pivot, and all elements in the right region are no less than the pivot:\n\n\n\n\nquicksort partition\n\n\n\nassuming that we have the partitioning function, quicksort then can be simply written as:\n\nvoid quicksort(int* a, int, from, int to)\n{\n    if (from &gt;= to) return; // recursion base\n    int p = partition (a, from, to);\n    quicksort(a, from, p); // recursive call on the left partition\n    quicksort(a, p + 1, to) // recursive call on the right partition\n}\n\nQuicksort Partition\n\nFirst pick an element from the range. This element is called the pivot. In the simplest version of quicksort the pivot is simply always chosen as the first element of the range. (This will lead to bad perfermance for sorted and almost sorted arrays)\nAssume that at some point of the execution we have the following state (this is the invariant of the partitioning algorithm) :\n\n\n\n\nquicksort partition invariant\n\n\n\nexecuting following actions while i &lt; j will preserve the invariant\ni++; while (a[i] &lt; pivot) i++;\nj--; while (a[j] &gt; pivot) j++;\nif (i &lt; j) {swap(a[i], a[j]);}\nto understand why consider the first two lines. After executing them following state holds:\n\n\n\nquicksort parition post-condition\n\n\nfinal line swaps a[i] and a[j], reastablishing the invariant.\nfull partitioning function is given as:\nint parition(int* a, int from, int to)\n{\n  int pivot = a[from]; // pivot is simply the first element\n  int i = from - 1;\n  int j = to + 1;\n  while (i &lt; j) {\n      i++; while (a[i] &lt; pivot) i++;\n      j--; while (a[j] &gt; pivot) j++;\n      if (i &lt; j) swap(a[i], a[j]);\n  }\n  return j;\n}\n\n\n\nQuicksort Naively\n\ndef quicksort(s) :\n    if len(s) &lt;= 1 : return s\n    p = s[len(s) // 2]\n    a = []\n    b = []\n    c = []\n    for i in range(0, len(s)) :\n        if s[i] &lt; p : a.append(s[i])\n    for i in range(0, len(s)) :\n        if s[i] == p : b.append(s[i])\n    for i in range(0, len(s)) :\n        if s[i] &gt; p : c.append(s[i])\n    return quicksort(a) + b + quicksort(c)\n\ntesting this naive implementation for some arrays:\n\nfor i in range (2, 8) :\n    randarr = np.random.randint(-10, 20, i)\n    print(\"in:  \", randarr)\n    print(\"out: \", quicksort(randarr))\n\nin:   [11  5]\nout:  [5, 11]\nin:   [ 7  4 19]\nout:  [4, 7, 19]\nin:   [18 10  5 14]\nout:  [5, 10, 14, 18]\nin:   [-1  0 19 -9 -1]\nout:  [-9, -1, -1, 0, 19]\nin:   [-5 -6  8 18 15 15]\nout:  [-6, -5, 8, 15, 15, 18]\nin:   [ 0 16  7 13 -7 -2 -4]\nout:  [-7, -4, -2, 0, 7, 13, 16]\n\n\n\n\nQuicksort Refinements\npseudocode:\nProcedure qSort(a : Array&lt;T&gt;; l, r : Nat) := \n    while r - l + 1 &gt; n0 :\n        j := pick_pivot_pos(a, l, r)\n        swap(a[l], a[j]) // pivot is at the first position\n        p := a[l]  // p is the value of the pivot\n        i := l; j := r\n        do\n            while a[i] &lt; p : i++; //skip over the elements \n            while a[j] &gt; p : j--; // already in the correct subarray\n            if i &lt;= j : \n                swap(a[i], a[j])\n                i++\n                j--\n        while i &lt;= j\n        qSort(a, l, j)\n        qSort(a, i, r)\ncpp implementation including testing for {3, 6, 8, 1, 0, 7, 2, 4, 5, 9}\n#include &lt;iostream&gt;\n\n// procedre for swapping integers\nvoid swap(int& x, int& y)\n{\n    int temp = x;\n    x = y;\n    y = temp;\n}\n\n// the partitioning function\n// simple version for pivot: always first element is chosen\nint partition(int* a, int from, int to)\n{\n    int pivot = a[from];\n    int i = from - 1;\n    int j = to + 1;\n    // invariant: a[f .. i] &lt;= pivot && a[j .. t] &gt;= pivot\n    while (i &lt; j) {\n        i++; while (a[i] &lt; pivot) i++;\n        j--; while (a[j] &gt; pivot) j--;\n        if (i &lt; j) swap(a[i], a[j]);\n    }\n    return j;\n}\n\n// quicksort itself\nvoid quicksort(int* a, int from, int to)\n{\n    if (from &gt;= to) return;\n    int p = partition(a, from, to);\n    quicksort(a, from, p);\n    quicksort(a, p + 1, to);\n}\n\n// testing quicksort:\nint main(int argc, const char** argv) {\n    int a[] = {3, 6, 8, 1, 0, 7, 2, 4, 5, 9};\n    quicksort(a, 0, 9);\n    for (int i = 0; i &lt; 10; i++)\n        std::cout &lt;&lt; a[i] &lt;&lt; \", \"; \n    std::cout &lt;&lt; a[9] &lt;&lt; std::endl;\n    return 0;\n}\n\n\nQuicksort Analysis\n\nAverage: \\(\\mathcal{O}(n\\log(n))\\)\nWorst-case: \\(\\mathcal{O}(n^2)\\). In the simplest case where the pivot is always chosen as the first element worst case unfortunately occurs whenever the input sorted or almost sorted. Since almost sorted inputs are quite common in practice other strategies for chosing the pivot are preffered.\nNevertheless by employing clever methods for chosing the pivot element we can almost always guarantee that quicksort runs in \\(\\mathcal{O}(n\\log(n))\\).\nTherefore in practice quicksort is preffered over mergesort.\n\n\n\n\nBucket Sort\nSo far in our model we assumed no information on keys; we didn’t know whether they are numbers, strings or any other data type. The only requirement was that any two keys where comparable. Our comparison based sorting algorithms relied solely on comparing any two keys. Theoretically it can be shown that the lower bound for such algorithms is \\(\\Omega{n\\log n}\\).\nNow if we extend our model\n\n# sorts keys in range [0, 100)\ndef KSort(s) :\n    # initialize orray of length 100 with empty buckets\n    b = []\n    for i in range(100) : b.append([])\n    # place elements in buckets\n    for el in s : b[el].append(el)\n    # array holding results\n    res = []\n    # append elements in buckets to res\n    for i in range(100) :\n        for el in b[i] : res.append(el)\n    return res\n\n# testing:\nfor i in range (2, 8) :\n    randarr = np.random.randint(0, 100, i)\n    print(\"in:  \", randarr)\n    print(\"out: \", KSort(randarr))\n\nin:   [72 78]\nout:  [72, 78]\nin:   [ 3 80 73]\nout:  [3, 73, 80]\nin:   [87 68 91 17]\nout:  [17, 68, 87, 91]\nin:   [90 66 80 25 77]\nout:  [25, 66, 77, 80, 90]\nin:   [66  2 45 86 40 26]\nout:  [2, 26, 40, 45, 66, 86]\nin:   [37 99 96 15  2 58 15]\nout:  [2, 15, 15, 37, 58, 96, 99]\n\n\n\nRadix Sort\nEmploying a clever trick we can significantly increase the range of keys. In bucket sort we perform the sorting on the key itself. In radix sort we iteratively perform bucket sort on the digits of the keys, starting from the least significant digit. This works especially because bucket sort is a stable sorting algorithm.\nThis way we can sort keys in range \\(10^d - 1\\). We have 10 buckets. Different bases can be chosen.\nWe slightly modify previous bucket sort, where a key function is passed as an argument, with \\(d = 5\\)\n\n# sorts keys in range [0, 10)\ndef KSort2(s, key) :\n    # initialize orray of length 100 with empty buckets\n    b = []\n    for i in range(10) : b.append([])\n    # place elements in buckets\n    for el in s : b[key(el)].append(el)\n    # array holding results\n    res = []\n    # append elements in buckets to res\n    for i in range(10) :\n        for el in b[i] : res.append(el)\n    return res\n\n# sorts keys in range [0, 10^5)\ndef LSDRadixSort(a) :\n    for i in range(5) :\n        a = KSort2(a, lambda x : (x // 10**i) % 10)\n    return a\n\n# testing \nfor i in range(5, 10) :\n    randarr = np.random.randint(0, 10**5, i)\n    print(\"input: \", randarr)\n    print(\"output: \", LSDRadixSort(randarr))\n\ninput:  [52805 76622 94183 79018 13734]\noutput:  [13734, 52805, 76622, 79018, 94183]\ninput:  [59667 86384 62886 20561 43297 25354]\noutput:  [20561, 25354, 43297, 59667, 62886, 86384]\ninput:  [46042 11610 59453 27738 95187 29479 56254]\noutput:  [11610, 27738, 29479, 46042, 56254, 59453, 95187]\ninput:  [92836  4667 12602 58412 27798 42578 89259 11778]\noutput:  [4667, 11778, 12602, 27798, 42578, 58412, 89259, 92836]\ninput:  [16937 92843 11417  6530 37633 81926 96655 76916 34755]\noutput:  [6530, 11417, 16937, 34755, 37633, 76916, 81926, 92843, 96655]",
    "crumbs": [
      "Data Structures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sorting and Priority Queues</span>"
    ]
  },
  {
    "objectID": "05/01-sorting.html#priority-queues-and-heap-data-structure",
    "href": "05/01-sorting.html#priority-queues-and-heap-data-structure",
    "title": "4  Sorting and Priority Queues",
    "section": "4.2 Priority Queues and Heap Data Structure",
    "text": "4.2 Priority Queues and Heap Data Structure\nA set \\(M\\) of Elements \\(e : T\\) with Keys supporting two operations:\n\ninsert(e): Insert \\(e\\) into \\(M\\).\ndelete_min(): remove the min element from \\(M\\) and return it.\n\n\nApplications\n\nGreedy algorithms (selecting the optimal local optimal solution)\nSimulation of discrete events\nbranch-and-bound search\ntime forward processing.\n\n\n\nBinary Heaps\nHeap Property:\n\nFor any leaf \\(a \\in M\\) \\(a\\) is a heap.\nLet \\(T_1,\\, T_2\\) be heaps. If \\(a \\leq x, \\, \\forall x\\in T_1, T_2\\), then \\(T_1 \\circ a \\circ T_2\\) is also a heap.\n\nComplete Binary Tree:\n\nA complete binary tree is a binary tree in which ever lebel, except possibly the last, is completely filled, and all nodes in the last level are as far left as possible.\n\nHeap:\n\nA heap is a complete binary tree that satisfies the heap property:\nA heap can be succinctly represented as an array:\n\n\n\n\nheap\n\n\n\nArray h[1..n]\nfor any given node with the number j:\n\nleft child: 2*j\nright child 2*j + 1\nparent: bottom(j/2)\n\n\nPseudocode:\nClass BinaryHeapPQ(capacity: Nat)&lt;T&gt; :=\n    h : Array[1..capacity]&lt;T&gt;\n    size := 0 : Nat // current amount of elements\n\n    // Heap-property\n    // invariant: h[bottom(j/2) &lt;= h(j)], for all j == 2..n\n\n    Function min() :=\n        assert size &gt; 0 // heap non-emtpy\n        return h[1]\n    \n    Procedure insert(e : T) :=\n        assert size &lt; capacity\n        size++\n        h[size] := e\n        siftUp(size)\n    \n    Procedure siftUp(i : Nat) := \n        // assert Heap-property violated at most at position i\n        if i == 1 or h[bottom(i / 2)] &lt;= h[i] then return\n        swap(h[i], h[bottom(i/2)])\n        siftUp(bottom(i/2))\n    \n    Procedure popMin : T :=\n        result = h[1] : T\n        h[1] := h[size]\n        size--\n        siftDown(1)\n        return result\n    \n    Procedure siftDown (i : Nat) :=\n        // assert: Heap property is at most at position 2*i or 2*i + 1 violated\n        if 2i &gt; n then return // i is a leaf\n\n        // select the appropriate child\n        if 2*i + 1 &gt; n or h[2*i] &lt;= h[2*i + 1] : \n        //no right child exists or left child is smaller than right\n            m := 2*i\n        else : m := 2*i + 1\n        if h[i] &gt; h[m] :\n            swap(h[i], h[m])\n            siftDown(m)\n\n    Procedure buildHeap(a[1..n]&lt;T&gt;) :=\n        h := a\n        buildRecursive(1)\n\n    Procedure buildHeapRecursive(i : Nat) :=\n        if 4*i &lt;= size : // children are not leaves\n            buildHeapRecursive(2*i) // assert: heap property holds for left subtree\n            buildHeaprecursive(2*i + 1) // assert: heap property holds for right subtree\n        siftDown(i) //assert Heap property holds for subtree starting at i \n    \n    //alternatively\n    Procudure buildHeapBackwards :=\n        for i := n/2 downto 1 :\n            siftDown(i)\n\n    Procedure heapSort(a[1..n]&lt;T&gt;) :=\n        buildHeap(a) // O(n)\n        for i := n downto 2 do : \n            h[i] := deleteMin(); // O(log(n))\n\nHeap Insert\nProcedure insert(e : T) :=\n    assert size &lt; capacity\n    size++\n    h[n] := e\n    siftUp(n)\n\nProcedure siftUp(i : Nat) := \n    // assert Heap-property violated at most at position i\n    if i == 1 or h[bottom(i / 2)] &lt;= h[i] then return\n    swap(h[i], h[bottom(i/2)])\n    siftUp(bottom(i/2))\nIllustration of heap insert:\n\n\n\nheap insert\n\n\n\n\nHeap Pop Min (or Delete Min)\n Procedure popMin : T :=\n        result = h[1] : T\n        h[1] := h[n]\n        n--\n        siftDown(1)\n        return result\n    \nProcedure siftDown (i : Nat)\n    // assert: Heap property is at most at position 2*i or 2*i + 1 violated\n    if 2i &gt; n then return // i is a leaf\n\n    // select the appropriate child\n    if 2*i + 1 &gt; n or h[2*i] &lt;= h[2*i + 1] : \n    //no right child exists or left child is smaller than right\n        m := 2*i\n   if h[i] &gt; h[m] :\n            swap(h[i], h[m])\n            siftDown(m)   else : m := 2*i + 1\nIllustration of pop min:\n\n\n\nheap pop min\n\n\n\n\nConstruction of a Binary Heap\n\nGiven are \\(n\\) numbers. Construct a heap from these numbers\nNaive Solution: \\(n\\) calls to insert() \\(\\Rightarrow\\) \\(\\mathcal{O}(n\\log(n))\\)\n\nProblem: If numbers are given in an array, we can’t perform the construction in place.\nIt is slow\n\nwe can do faster and in place in \\(\\mathcal{O}(n)\\) time.\n\nPseudocode for recursive implementation:\n Procedure buildHeap(a[1..n] : T) :=\n        h := a\n        buildRecursive(1)\n\nProcedure buildHeapRecursive(i : Nat) :=\n    if 4*i &lt;= n : // children are not leaves\n        buildHeapRecursive(2*i) // assert: heap property holds for left subtree\n        buildHeaprecursive(2*i + 1) // assert: heap property holds for right subtree\n    siftDown(i) //assert Heap property holds for subtree starting at i \nA simpler iterative one-liner:\n Procudure buildHeapBackwards :=\n        for i := n/2 downto 1 :\n            siftDown(i)\n\\(\\lfloor i/2 \\rfloor\\) is the last non-leaf node.\nTime complexity of these binary heap construction algorithms is \\(\\mathbfcal{O}(n)\\).\n\n\nHeapsort\nProcedure heapSort(a[1..n]&lt;T&gt;) :=\n        buildHeap(a) // O(n)\n        for i := n downto 2 do : \n            h[i] := deleteMin(); // O(log(n))\nSorts in decreasing order in \\(\\mathcal{O}(n\\log(n))\\), by removing the minimal element and writing the return value to the end of the array in place.",
    "crumbs": [
      "Data Structures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sorting and Priority Queues</span>"
    ]
  }
]